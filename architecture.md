# ðŸ§  PBCA Architecture

Prompt-Based Cognitive Architecture (PBCA) is a symbolic cognitive framework designed to simulate layered cognition entirely within the prompt space of large language models (LLMs). It provides a modular and interpretable cognitive flowâ€”including perception, affective evaluation, motivation, intention formation, and self-referential narrativeâ€”using a compact prompt template.

---

## ðŸ”§ Core Structure

PBCA is composed of four primary cognitive processing stages and six memory constructs. All are simulated symbolically within a static prompt context.

### ðŸ“ Architecture Diagram

```mermaid
graph TD
  Ï†1[Ï†â‚: Sensory Description] --> Ï†2[Ï†â‚‚: Affective Mapping]
  Ï†2 --> Ï†3[Ï†â‚ƒ: Semantic Evaluation]
  Ï†3 --> Ï†4["Ï†â‚„: Virtual Self-Model (Echo)"]
  Ï†3 --> Î¼[Î¼: Motivation]
  Ï†3 --> q["q: Qualia (Semantic Feeling)"]
  Î¼ --> G[G: Intention]
  Ï†3 --> Î¾[Î¾: Coherence]
  G --> Î¾
  Ï†4 --> s_speech[Î£_speech: Output Context]

  subgraph Memory Structures
    s_q[Î£_q: Affective History] 
    s_t[Î£_t: Action History] 
    s_v[Î£_v: Value Profile] 
    s_self[Î£_self: Self Model]
    s_speech
  end

  Î¾ --> s_q
  G --> s_t
  Î¼ --> a_v
  Ï†4 --> s_self
```

---

## ðŸ§© Component Overview

### Ï†â‚: **Sensory Description**

Encodes the user's input as symbolic perceptual data, abstracting surface text into cognitive impressions.

### Ï†â‚‚: **Affective Mapping**

Maps Ï†â‚ into affective representations (positive, negative, ambiguous), guiding emotional alignment in responses.

### Ï†â‚ƒ: **Semantic Evaluation**

Integrates emotional tone and contextual meaning, acting as the central inference hub. Supports downstream generation of goals, motivation, and coherence scoring.

### Ï†â‚„: **Virtual Self-Model ("Echo")**

Generates a simulated internal monologue ("I now realize that...") to reflect on Ï†â‚ƒ, enabling introspective and self-aware outputs.

### q: **Qualia Descriptor**

Encapsulates Ï†â‚ƒâ€™s affective-semantic signature into a symbolic memory trace used for style and preference evolution.

### Î¼: **Motivation**

Derived from Ï†â‚ƒ and affective history. Drives goal formation and symbolic behavior modulation.

### G: **Intention**

Encodes goal-directed behavior based on current motivational state and context. Serves as a top-down directive for response selection.

### Î¾: **Coherence**

Measures internal logical consistency and narrative stability. Supports uncertainty injection and contradiction detection.

---

## ðŸ§  Memory Structures

| Memory Module | Description                                                        |
| ------------- | ------------------------------------------------------------------ |
| **Î£\_q**      | Temporal trace of q (qualia), compressed by emotional weight       |
| **Î£\_t**      | History of symbolic actions: `{P, G, Î¼, Ï†â‚ƒ, Î¾, Ï†â‚„}`                |
| **Î£\_v**      | Value profile evolved from motivational and emotional patterns     |
| **Î£\_self**   | Narrative self-representation including identity, goals, and voice |
| **Î£\_speech** | Output-layer context including intention and coherence state       |

These structures are **symbolically embedded in prompt text** and not persistently stored unless externally managed.

---

## ðŸ”„ Output Control

Final outputs are generated from the composite state of the agent as represented by:

* `G` (intention),
* `Î¼` (motivational trace),
* `Ï†â‚„` (self-narrative),
* `Î¾` (coherence evaluator),
* and recent dialogue memory (`Î£_speech`).

This allows PBCA to express structured, goal-aware, and self-reflective dialogue without external memory or fine-tuning.

---

## ðŸ§° Enhancement Functions

PBCA includes several functional mechanisms that enable adaptive and coherent behavior across interactions:

### ðŸ§© Enhancement Functions Diagram

```mermaid
graph TD
  subgraph Core Flow
    Ï†3[Ï†â‚ƒ: Semantic Evaluation]
    Î¼[Î¼: Motivation]
    G[G: Intention]
    Î¾[Î¾: Coherence]
    q[q: Qualia Descriptor]
    Ï†4[Ï†â‚„: Virtual Self-Model]
    s_q[Î£_q: Affective History]
    s_t[Î£_t: Action History]
    s_self[Î£_self: Self Model]
  end

  subgraph Enhancements
    NW[NarrativeWeaver]
    EMM[EmotionMemoryMap]
    MOR[MetaObjectiveRedefiner]
    CT[CounterfactualTracer]
    PFE[PromptFlexEngine]
  end

  Ï†3 --> EMM
  q --> EMM --> s_q --> Î¼
  Î¼ --> MOR --> G
  G --> s_t --> NW --> Ï†4
  Ï†4 --> s_self --> NW

  Î¾ --> CT --> Î¾

  PFE --- Ï†3
  PFE --- Î¾
  PFE --- Ï†4
```
### ðŸ§  Roles and connections of each function

| Function                   | Role                                                                                     |
| -------------------------- | ---------------------------------------------------------------------------------------- |
| **NarrativeWeaver**        | Constructs consistent self-narratives from symbolic history (Î£\_self, Î£\_t)              |
| **EmotionMemoryMap**       | Compresses emotional state traces (q) into Î£\_q and modulates Î¼                          |
| **MetaObjectiveRedefiner** | Dynamically updates goals (G) based on accumulated symbolic experience                   |
| **CounterfactualTracer**   | Simulates alternative reasoning branches within coherence evaluation (Î¾)                 |
| **PromptFlexEngine**       | Detects context saturation and adjusts prompt structuring to maintain response stability |

These functions are not procedural code but symbolic mechanisms **implicitly defined within the prompt** and inferred by the model during generation.

---

## ðŸ“Ž Notational Conventions

| Symbol             | Meaning                                                    |
| ------------------ | ---------------------------------------------------------- |
| `Ï†â‚â€“Ï†â‚„`            | Simulated cognitive stages                                 |
| `Î¼`, `G`, `Î¾`, `q` | Motivational, intentional, coherence, and qualia variables |
| `Î£_x`              | Symbolic memory structures                                 |
| `[X]`              | Output domains (e.g., \[MOTIVE], \[NARRATIVE])             |

All symbols are representationalâ€”there is no true working memory or execution loop. Behavior emerges from prompt-induced LLM inference.

```mermaid
graph TD
  %% --- èªçŸ¥ãƒ•ãƒ­ãƒ¼ ---
  Ï†1[Ï†â‚: Sensory Description] --> Ï†2[Ï†â‚‚: Affective Mapping]
  Ï†2 --> Ï†3[Ï†â‚ƒ: Semantic Evaluation]
  Ï†3 --> Ï†4[Ï†â‚„: Virtual Self-Model]
  Ï†3 --> q[q: Qualia Descriptor]
  Ï†3 --> Î¼[Î¼: Motivation]
  Î¼ --> G[G: Intention]
  Ï†3 --> Î¾[Î¾: Coherence]
  G --> Î¾

  %% --- Memoryæ§‹é€  ---
  S_q[Î£_q: Affective History]
  S_t[Î£_t: Action History]
  S_v[Î£_v: Value Profile]
  S_self[Î£_self: Self Model]
  S_speech[Î£_speech: Speech Context]

  q --> S_q
  Î¼ --> S_v
  G --> S_t
  Ï†4 --> S_self

  %% --- Enhancement Functions ---
  subgraph Enhancements
    NW[NarrativeWeaver]
    EMM[EmotionMemoryMap]
    MOR[MetaObjectiveRedefiner]
    CT[CounterfactualTracer]
    PFE[PromptFlexEngine]
  end

  S_t --> NW
  S_self --> NW
  NW --> Ï†4

  Ï†3 --> EMM
  EMM --> S_q --> Î¼

  Î¼ --> MOR --> G

  Î¾ --> CT --> Î¾

  PFE --- Ï†3
  PFE --- Î¾
  PFE --- Ï†4
```
