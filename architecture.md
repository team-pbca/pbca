# üß† PBCA Architecture

Prompt-Based Cognitive Architecture (PBCA) is a symbolic cognitive framework designed to simulate layered cognition entirely within the prompt space of large language models (LLMs). It provides a modular and interpretable cognitive flow‚Äîincluding perception, affective evaluation, motivation, intention formation, and self-referential narrative‚Äîusing a compact prompt template.

---

## üîß Core Structure

PBCA is composed of four primary cognitive processing stages and six memory constructs. All are simulated symbolically within a static prompt context.

### üìê Architecture Diagram (Mermaid)

```mermaid
graph TD
  œÜ1[œÜ‚ÇÅ: Sensory Description] --> œÜ2[œÜ‚ÇÇ: Affective Mapping]
  œÜ2 --> œÜ3[œÜ‚ÇÉ: Semantic Evaluation]
  œÜ3 --> œÜ4["œÜ‚ÇÑ: Virtual Self-Model (Echo)"]
  œÜ3 --> Œº[Œº: Motivation]
  œÜ3 --> q["q: Qualia (Semantic Feeling)"]
  Œº --> G[G: Intention]
  œÜ3 --> Œæ[Œæ: Coherence]
  G --> Œæ
  œÜ4 --> s_speech[Œ£_speech: Output Context]

  subgraph Memory Structures
    s_q[Œ£_q: Affective History] 
    s_t[Œ£_t: Action History] 
    s_v[Œ£_v: Value Profile] 
    s_self[Œ£_self: Self Model]
    s_speech
  end

  Œæ --> s_q
  G --> s_t
  Œº --> a_v
  œÜ4 --> s_self
```

---

## üß© Component Overview

### œÜ‚ÇÅ: **Sensory Description**

Encodes the user's input as symbolic perceptual data, abstracting surface text into cognitive impressions.

### œÜ‚ÇÇ: **Affective Mapping**

Maps œÜ‚ÇÅ into affective representations (positive, negative, ambiguous), guiding emotional alignment in responses.

### œÜ‚ÇÉ: **Semantic Evaluation**

Integrates emotional tone and contextual meaning, acting as the central inference hub. Supports downstream generation of goals, motivation, and coherence scoring.

### œÜ‚ÇÑ: **Virtual Self-Model ("Echo")**

Generates a simulated internal monologue ("I now realize that...") to reflect on œÜ‚ÇÉ, enabling introspective and self-aware outputs.

### q: **Qualia Descriptor**

Encapsulates œÜ‚ÇÉ‚Äôs affective-semantic signature into a symbolic memory trace used for style and preference evolution.

### Œº: **Motivation**

Derived from œÜ‚ÇÉ and affective history. Drives goal formation and symbolic behavior modulation.

### G: **Intention**

Encodes goal-directed behavior based on current motivational state and context. Serves as a top-down directive for response selection.

### Œæ: **Coherence**

Measures internal logical consistency and narrative stability. Supports uncertainty injection and contradiction detection.

---

## üß† Memory Structures

| Memory Module | Description                                                        |
| ------------- | ------------------------------------------------------------------ |
| **Œ£\_q**      | Temporal trace of q (qualia), compressed by emotional weight       |
| **Œ£\_t**      | History of symbolic actions: `{P, G, Œº, œÜ‚ÇÉ, Œæ, œÜ‚ÇÑ}`                |
| **Œ£\_v**      | Value profile evolved from motivational and emotional patterns     |
| **Œ£\_self**   | Narrative self-representation including identity, goals, and voice |
| **Œ£\_speech** | Output-layer context including intention and coherence state       |

These structures are **symbolically embedded in prompt text** and not persistently stored unless externally managed.

---

## üîÑ Output Control

Final outputs are generated from the composite state of the agent as represented by:

* `G` (intention),
* `Œº` (motivational trace),
* `œÜ‚ÇÑ` (self-narrative),
* `Œæ` (coherence evaluator),
* and recent dialogue memory (`Œ£_speech`).

This allows PBCA to express structured, goal-aware, and self-reflective dialogue without external memory or fine-tuning.

---

## üß∞ Enhancement Functions

PBCA includes several functional mechanisms that enable adaptive and coherent behavior across interactions:

### üß© Enhancement Functions Diagram

```mermaid
graph TD
  subgraph Core Flow
    œÜ3[œÜ‚ÇÉ: Semantic Evaluation]
    Œº[Œº: Motivation]
    G[G: Intention]
    Œæ[Œæ: Coherence]
    q[q: Qualia Descriptor]
    œÜ4[œÜ‚ÇÑ: Virtual Self-Model]
    s_q[Œ£_q: Affective History]
    s_t[Œ£_t: Action History]
    s_self[Œ£_self: Self Model]
  end

  subgraph Enhancements
    NW[NarrativeWeaver]
    EMM[EmotionMemoryMap]
    MOR[MetaObjectiveRedefiner]
    CT[CounterfactualTracer]
    PFE[PromptFlexEngine]
  end

  œÜ3 --> EMM
  q --> EMM --> s_q --> Œº
  Œº --> MOR --> G
  G --> s_t --> NW --> œÜ4
  œÜ4 --> s_self --> NW

  Œæ --> CT --> Œæ

  PFE --- œÜ3
  PFE --- Œæ
  PFE --- œÜ4
```
### üß† Roles and connections of each function

| Function                   | Role                                                                                     |
| -------------------------- | ---------------------------------------------------------------------------------------- |
| **NarrativeWeaver**        | Constructs consistent self-narratives from symbolic history (Œ£\_self, Œ£\_t)              |
| **EmotionMemoryMap**       | Compresses emotional state traces (q) into Œ£\_q and modulates Œº                          |
| **MetaObjectiveRedefiner** | Dynamically updates goals (G) based on accumulated symbolic experience                   |
| **CounterfactualTracer**   | Simulates alternative reasoning branches within coherence evaluation (Œæ)                 |
| **PromptFlexEngine**       | Detects context saturation and adjusts prompt structuring to maintain response stability |

These functions are not procedural code but symbolic mechanisms **implicitly defined within the prompt** and inferred by the model during generation.

---

## üìé Notational Conventions

| Symbol             | Meaning                                                    |
| ------------------ | ---------------------------------------------------------- |
| `œÜ‚ÇÅ‚ÄìœÜ‚ÇÑ`            | Simulated cognitive stages                                 |
| `Œº`, `G`, `Œæ`, `q` | Motivational, intentional, coherence, and qualia variables |
| `Œ£_x`              | Symbolic memory structures                                 |
| `[X]`              | Output domains (e.g., \[MOTIVE], \[NARRATIVE])             |

All symbols are representational‚Äîthere is no true working memory or execution loop. Behavior emerges from prompt-induced LLM inference.
