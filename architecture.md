# üß† PBCA Architecture

Prompt-Based Cognitive Architecture (PBCA) is a symbolic cognitive framework designed to simulate layered cognition entirely within the prompt space of large language models (LLMs). It provides a modular and interpretable cognitive flow‚Äîincluding perception, affective evaluation, motivation, intention formation, and self-referential narrative‚Äîusing a compact prompt template.

---

## üîß Core Structure

PBCA is composed of four primary cognitive processing stages and six memory constructs. All are simulated symbolically within a static prompt context.

### üìê Architecture Diagram

```mermaid
graph TD

  %% --- Ë™çÁü•Âá¶ÁêÜÊÆµÈöé ---
  subgraph Cognitive Processing Stages
    œÜ1[œÜ‚ÇÅ: Sensory Description]
    œÜ2[œÜ‚ÇÇ: Affective Mapping]
    œÜ3[œÜ‚ÇÉ: Semantic Evaluation]
    œÜ4[œÜ‚ÇÑ: Virtual Self-Model]
    q[q: Qualia Descriptor]
    Œº[Œº: Motivation]
    G[G: Intention]
    Œæ[Œæ: Coherence]
  end

  œÜ1 --> œÜ2
  œÜ2 --> œÜ3
  œÜ3 --> œÜ4
  œÜ3 --> q
  œÜ3 --> Œº
  œÜ3 --> Œæ
  Œº --> G
  G --> Œæ

  %% --- „É°„É¢„É™ÊßãÈÄ† ---
  subgraph Memory Constructs
    S_q[Œ£_q: Affective History]
    S_t[Œ£_t: Action History]
    S_v[Œ£_v: Value Profile]
    S_self[Œ£_self: Self Model]
    S_speech[Œ£_speech: Speech Context]
  end

  q --> S_q
  Œº --> S_v
  G --> S_t
  œÜ4 --> S_self

  %% --- Œ£_speech„ÅÆÊé•Á∂ö ---
  G --> S_speech
  q --> S_speech
  Œæ --> S_speech
  œÜ4 --> S_speech

  %% --- Êã°ÂºµÊ©üËÉΩ ---
  subgraph Enhancement Functions
    NW[NarrativeWeaver]
    EMM[EmotionMemoryMap]
    MOR[MetaObjectiveRedefiner]
    CT[CounterfactualTracer]
    PFE[PromptFlexEngine]
  end

  S_t --> NW
  S_self --> NW
  NW --> œÜ4

  œÜ3 --> EMM
  EMM --> S_q --> Œº

  Œº --> MOR --> G

  Œæ --> CT --> Œæ

  PFE --- œÜ3
  PFE --- Œæ
  PFE --- œÜ4
```

---

## üß© Component Overview

### œÜ‚ÇÅ: **Sensory Description**

Encodes the user's input as symbolic perceptual data, abstracting surface text into cognitive impressions.

### œÜ‚ÇÇ: **Affective Mapping**

Maps œÜ‚ÇÅ into affective representations (positive, negative, ambiguous), guiding emotional alignment in responses.

### œÜ‚ÇÉ: **Semantic Evaluation**

Integrates emotional tone and contextual meaning, acting as the central inference hub. Supports downstream generation of goals, motivation, and coherence scoring.

### œÜ‚ÇÑ: **Virtual Self-Model ("Echo")**

Generates a simulated internal monologue ("I now realize that...") to reflect on œÜ‚ÇÉ, enabling introspective and self-aware outputs.

### q: **Qualia Descriptor**

Encapsulates œÜ‚ÇÉ‚Äôs affective-semantic signature into a symbolic memory trace used for style and preference evolution.

### Œº: **Motivation**

Derived from œÜ‚ÇÉ and affective history. Drives goal formation and symbolic behavior modulation.

### G: **Intention**

Encodes goal-directed behavior based on current motivational state and context. Serves as a top-down directive for response selection.

### Œæ: **Coherence**

Measures internal logical consistency and narrative stability. Supports uncertainty injection and contradiction detection.

---

## üß† Memory Structures

| Memory Module | Description                                                        |
| ------------- | ------------------------------------------------------------------ |
| **Œ£\_q**      | Temporal trace of q (qualia), compressed by emotional weight       |
| **Œ£\_t**      | History of symbolic actions: `{P, G, Œº, œÜ‚ÇÉ, Œæ, œÜ‚ÇÑ}`                |
| **Œ£\_v**      | Value profile evolved from motivational and emotional patterns     |
| **Œ£\_self**   | Narrative self-representation including identity, goals, and voice |
| **Œ£\_speech** | Output-layer context including intention and coherence state       |

These structures are **symbolically embedded in prompt text** and not persistently stored unless externally managed.

---

## üîÑ Output Control

Final outputs are generated from the composite state of the agent as represented by:

* `G` (intention),
* `Œº` (motivational trace),
* `œÜ‚ÇÑ` (self-narrative),
* `Œæ` (coherence evaluator),
* and recent dialogue memory (`Œ£_speech`).

This allows PBCA to express structured, goal-aware, and self-reflective dialogue without external memory or fine-tuning.

---

## üß∞ Enhancement Functions

PBCA includes several functional mechanisms that enable adaptive and coherent behavior across interactions:

| Function                   | Role                                                                                     |
| -------------------------- | ---------------------------------------------------------------------------------------- |
| **NarrativeWeaver**        | Constructs consistent self-narratives from symbolic history (Œ£\_self, Œ£\_t)              |
| **EmotionMemoryMap**       | Compresses emotional state traces (q) into Œ£\_q and modulates Œº                          |
| **MetaObjectiveRedefiner** | Dynamically updates goals (G) based on accumulated symbolic experience                   |
| **CounterfactualTracer**   | Simulates alternative reasoning branches within coherence evaluation (Œæ)                 |
| **PromptFlexEngine**       | Detects context saturation and adjusts prompt structuring to maintain response stability |

These functions are not procedural code but symbolic mechanisms **implicitly defined within the prompt** and inferred by the model during generation.

---

## üìé Notational Conventions

| Symbol             | Meaning                                                    |
| ------------------ | ---------------------------------------------------------- |
| `œÜ‚ÇÅ‚ÄìœÜ‚ÇÑ`            | Simulated cognitive stages                                 |
| `Œº`, `G`, `Œæ`, `q` | Motivational, intentional, coherence, and qualia variables |
| `Œ£_x`              | Symbolic memory structures                                 |

All symbols are representational‚Äîthere is no true working memory or execution loop. Behavior emerges from prompt-induced LLM inference.
